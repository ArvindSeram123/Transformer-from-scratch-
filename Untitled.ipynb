{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 375,
   "id": "5740aff5-79c2-46a5-9a0f-260550c64c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import Dataset,DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "id": "0016472a-185d-4a1f-b0af-749b6a5f72a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "id": "9869de0d-440e-4485-af77-a4d8eaf56250",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HuggingFace linraries\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer\n",
    "from tokenizers.models import WordLevel\n",
    "from tokenizers.trainers import WordLevelTrainer\n",
    "from tokenizers.pre_tokenizers import Whitespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "82415f2b-ec4a-478f-b15a-e21207df89d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pathlib\n",
    "from pathlib import Path\n",
    "#typing\n",
    "from typing import Any\n",
    "#Library for progress bars in loops\n",
    "from tqdm import tqdm\n",
    "#importing library of warnings\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "a73b690d-5ea6-43a3-a466-6453d282583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Input Embeddings\n",
    "class InputEmbeddings(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, vocab_size: int):\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimension of vectors (512)\n",
    "        self.vocab_size = vocab_size # Size of the vocabulary\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model) # PyTorch layer that converts integer indices to dense embeddings\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.embedding(x) * math.sqrt(self.d_model) # Normalizing the variance of the embedding\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "2e6a042d-b0a0-4e95-b3e5-6848163b17d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Positional Encoding\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, seq_len: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.d_model = d_model # Dimensionality of the model\n",
    "        self.seq_len = seq_len # Maximum sequence length\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to prevent overfitting\n",
    "\n",
    "        # Creating a positional encoding matrix of shape (seq_len, d_model) filled with zeros\n",
    "        pe = torch.zeros(seq_len, d_model)\n",
    "\n",
    "        # Creating a tensor representing positions (0 to seq_len - 1)\n",
    "        position = torch.arange(0, seq_len, dtype = torch.float).unsqueeze(1) # Transforming 'position' into a 2D tensor['seq_len, 1']\n",
    "\n",
    "        # Creating the division term for the positional encoding formula\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in pe\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in pe\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Adding an extra dimension at the beginning of pe matrix for batch handling\n",
    "        pe = pe.unsqueeze(0)\n",
    "\n",
    "        # Registering 'pe' as buffer. Buffer is a tensor not considered as a model parameter\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self,x):\n",
    "        # Addind positional encoding to the input tensor X\n",
    "        x = x + (self.pe[:, :x.shape[1], :]).requires_grad_(False)\n",
    "        return self.dropout(x) # Dropout for regularization\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "d16aa857-b6b2-4521-a6d3-daa2295b0575",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Layer Normalization\n",
    "class LayerNormalization(nn.Module):\n",
    "\n",
    "    def __init__(self, eps: float = 10**-6) -> None: # We define epsilon as 0.000001 to avoid division by zero\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "\n",
    "        # We define alpha as a trainable parameter and initialize it with ones\n",
    "        self.alpha = nn.Parameter(torch.ones(1)) # One-dimensional tensor that will be used to scale the input data\n",
    "\n",
    "        # We define bias as a trainable parameter and initialize it with zeros\n",
    "        self.bias = nn.Parameter(torch.zeros(1)) # One-dimensional tenso that will be added to the input data\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean = x.mean(dim = -1, keepdim = True) # Computing the mean of the input data. Keeping the number of dimensions unchanged\n",
    "        std = x.std(dim = -1, keepdim = True) # Computing the standard deviation of the input data. Keeping the number of dimensions unchanged\n",
    "\n",
    "        # Returning the normalized input\n",
    "        return self.alpha * (x-mean) / (std + self.eps) + self.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8e57c5c0-eb94-4f91-9a9b-5eada56f8226",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Creating Feed Forward Layers\n",
    "class FeedForwardBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, d_ff: int, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # First linear transformation\n",
    "        self.linear_1 = nn.Linear(d_model, d_ff) # W1 & b1\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout to prevent overfitting\n",
    "        # Second linear transformation\n",
    "        self.linear_2 = nn.Linear(d_ff, d_model) # W2 & b2\n",
    "\n",
    "    def forward(self, x):\n",
    "        # (Batch, seq_len, d_model) --> (batch, seq_len, d_ff) -->(batch, seq_len, d_model)\n",
    "        return self.linear_2(self.dropout(torch.relu(self.linear_1(x))))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "770f1582-f0aa-4a4b-86eb-4d6576beab0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Multi-Head Attention block\n",
    "class MultiHeadAttentionBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, d_model: int, h: int, dropout: float) -> None: # h = number of heads\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "\n",
    "        # We ensure that the dimensions of the model is divisible by the number of heads\n",
    "        assert d_model % h == 0, 'd_model is not divisible by h'\n",
    "\n",
    "        # d_k is the dimension of each attention head's key, query, and value vectors\n",
    "        self.d_k = d_model // h # d_k formula, like in the original \"Attention Is All You Need\" paper\n",
    "\n",
    "        # Defining the weight matrices\n",
    "        self.w_q = nn.Linear(d_model, d_model) # W_q\n",
    "        self.w_k = nn.Linear(d_model, d_model) # W_k\n",
    "        self.w_v = nn.Linear(d_model, d_model) # W_v\n",
    "        self.w_o = nn.Linear(d_model, d_model) # W_o\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout) # Dropout layer to avoid overfitting\n",
    "\n",
    "\n",
    "    @staticmethod\n",
    "    def attention(query, key, value, mask, dropout: nn.Dropout):# mask => When we want certain words to NOT interact with others, we \"hide\" them\n",
    "\n",
    "        d_k = query.shape[-1] # The last dimension of query, key, and value\n",
    "\n",
    "        # We calculate the Attention(Q,K,V) as in the formula in the image above\n",
    "        attention_scores = (query @ key.transpose(-2,-1)) / math.sqrt(d_k) # @ = Matrix multiplication sign in PyTorch\n",
    "\n",
    "        # Before applying the softmax, we apply the mask to hide some interactions between words\n",
    "        if mask is not None: # If a mask IS defined...\n",
    "            attention_scores.masked_fill_(mask == 0, -1e9) # Replace each value where mask is equal to 0 by -1e9\n",
    "        attention_scores = attention_scores.softmax(dim = -1) # Applying softmax\n",
    "        if dropout is not None: # If a dropout IS defined...\n",
    "            attention_scores = dropout(attention_scores) # We apply dropout to prevent overfitting\n",
    "\n",
    "        return (attention_scores @ value), attention_scores # Multiply the output matrix by the V matrix, as in the formula\n",
    "\n",
    "    def forward(self, q, k, v, mask):\n",
    "\n",
    "        query = self.w_q(q) # Q' matrix\n",
    "        key = self.w_k(k) # K' matrix\n",
    "        value = self.w_v(v) # V' matrix\n",
    "\n",
    "\n",
    "        # Splitting results into smaller matrices for the different heads\n",
    "        # Splitting embeddings (third dimension) into h parts\n",
    "        query = query.view(query.shape[0], query.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        key = key.view(key.shape[0], key.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "        value = value.view(value.shape[0], value.shape[1], self.h, self.d_k).transpose(1,2) # Transpose => bring the head to the second dimension\n",
    "\n",
    "        # Obtaining the output and the attention scores\n",
    "        x, self.attention_scores = MultiHeadAttentionBlock.attention(query, key, value, mask, self.dropout)\n",
    "\n",
    "        # Obtaining the H matrix\n",
    "        x = x.transpose(1, 2).contiguous().view(x.shape[0], -1, self.h * self.d_k)\n",
    "\n",
    "        return self.w_o(x) # Multiply the H matrix by the weight matrix W_o, resulting in the MH-A matrix\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "1e3e3c22-5ec0-42f7-8482-39a9305aae60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Residual Connection\n",
    "class ResidualConnection(nn.Module):\n",
    "    def __init__(self, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout) # We use a dropout layer to prevent overfitting\n",
    "        self.norm = LayerNormalization() # We use a normalization layer\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # We normalize the input and add it to the original input 'x'. This creates the residual connection process.\n",
    "        return x + self.dropout(sublayer(self.norm(x)))\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "fbcefcea-ce23-4e30-a20c-8aa2af378f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Encoder Block\n",
    "class EncoderBlock(nn.Module):\n",
    "\n",
    "    # This block takes in the MultiHeadAttentionBlock and FeedForwardBlock, as well as the dropout rate for the residual connections\n",
    "    def __init__(self, self_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        # Storing the self-attention block and feed-forward block\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(2)]) # 2 Residual Connections with dropout\n",
    "\n",
    "    def forward(self, x, src_mask):\n",
    "        # Applying the first residual connection with the self-attention block\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, src_mask)) # Three 'x's corresponding to query, key, and value inputs plus source mask\n",
    "\n",
    "        # Applying the second residual connection with the feed-forward block\n",
    "        x = self.residual_connections[1](x, self.feed_forward_block)\n",
    "        return x # Output tensor after applying self-attention and feed-forward layers with residual connections.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "cdd6a458-dd6e-4af6-a703-eafac910c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Encoder\n",
    "# An Encoder can have several Encoder Blocks\n",
    "class Encoder(nn.Module):\n",
    "\n",
    "    # The Encoder takes in instances of 'EncoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "        self.layers = layers # Storing the EncoderBlocks\n",
    "        self.norm = LayerNormalization() # Layer for the normalization of the output of the encoder layers\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # Iterating over each EncoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask) # Applying each EncoderBlock to the input tensor 'x'\n",
    "        return self.norm(x) # Normalizing output\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "cf8d2281-20f0-4865-b6b2-05f646468360",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decoder Block\n",
    "class DecoderBlock(nn.Module):\n",
    "\n",
    "    # The DecoderBlock takes in two MultiHeadAttentionBlock. One is self-attention, while the other is cross-attention.\n",
    "    # It also takes in the feed-forward block and the dropout rate\n",
    "    def __init__(self,  self_attention_block: MultiHeadAttentionBlock, cross_attention_block: MultiHeadAttentionBlock, feed_forward_block: FeedForwardBlock, dropout: float) -> None:\n",
    "        super().__init__()\n",
    "        self.self_attention_block = self_attention_block\n",
    "        self.cross_attention_block = cross_attention_block\n",
    "        self.feed_forward_block = feed_forward_block\n",
    "        self.residual_connections = nn.ModuleList([ResidualConnection(dropout) for _ in range(3)]) # List of three Residual Connections with dropout rate\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Self-Attention block with query, key, and value plus the target language mask\n",
    "        x = self.residual_connections[0](x, lambda x: self.self_attention_block(x, x, x, tgt_mask))\n",
    "\n",
    "        # The Cross-Attention block using two 'encoder_ouput's for key and value plus the source language mask. It also takes in 'x' for Decoder queries\n",
    "        x = self.residual_connections[1](x, lambda x: self.cross_attention_block(x, encoder_output, encoder_output, src_mask))\n",
    "\n",
    "        # Feed-forward block with residual connections\n",
    "        x = self.residual_connections[2](x, self.feed_forward_block)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "3f6166ad-6e43-4938-abfc-6ee5c00d4532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Decoder\n",
    "# A Decoder can have several Decoder Blocks\n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    # The Decoder takes in instances of 'DecoderBlock'\n",
    "    def __init__(self, layers: nn.ModuleList) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        # Storing the 'DecoderBlock's\n",
    "        self.layers = layers\n",
    "        self.norm = LayerNormalization() # Layer to normalize the output\n",
    "\n",
    "    def forward(self, x, encoder_output, src_mask, tgt_mask):\n",
    "\n",
    "        # Iterating over each DecoderBlock stored in self.layers\n",
    "        for layer in self.layers:\n",
    "            # Applies each DecoderBlock to the input 'x' plus the encoder output and source and target masks\n",
    "            x = layer(x, encoder_output, src_mask, tgt_mask)\n",
    "        return self.norm(x) # Returns normalized output\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "d9813ce1-f245-439f-98b1-fec00471ac1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Buiding Linear Layer\n",
    "class ProjectionLayer(nn.Module):\n",
    "    def __init__(self, d_model: int, vocab_size: int) -> None: # Model dimension and the size of the output vocabulary\n",
    "        super().__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab_size) # Linear layer for projecting the feature space of 'd_model' to the output space of 'vocab_size'\n",
    "    def forward(self, x):\n",
    "        return torch.log_softmax(self.proj(x), dim = -1) # Applying the log Softmax function to the output\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "586aeded-451d-4434-aa4c-1cc4f781ec04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the Transformer Architecture\n",
    "class Transformer(nn.Module):\n",
    "\n",
    "    # This takes in the encoder and decoder, as well the embeddings for the source and target language.\n",
    "    # It also takes in the Positional Encoding for the source and target language, as well as the projection layer\n",
    "    def __init__(self, encoder: Encoder, decoder: Decoder, src_embed: InputEmbeddings, tgt_embed: InputEmbeddings, src_pos: PositionalEncoding, tgt_pos: PositionalEncoding, projection_layer: ProjectionLayer) -> None:\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.src_pos = src_pos\n",
    "        self.tgt_pos = tgt_pos\n",
    "        self.projection_layer = projection_layer\n",
    "\n",
    "    # Encoder\n",
    "    def encode(self, src, src_mask):\n",
    "        src = self.src_embed(src) # Applying source embeddings to the input source language\n",
    "        src = self.src_pos(src) # Applying source positional encoding to the source embeddings\n",
    "        return self.encoder(src, src_mask) # Returning the source embeddings plus a source mask to prevent attention to certain elements\n",
    "\n",
    "    # Decoder\n",
    "    def decode(self, encoder_output, src_mask, tgt, tgt_mask):\n",
    "        tgt = self.tgt_embed(tgt) # Applying target embeddings to the input target language (tgt)\n",
    "        tgt = self.tgt_pos(tgt) # Applying target positional encoding to the target embeddings\n",
    "\n",
    "        # Returning the target embeddings, the output of the encoder, and both source and target masks\n",
    "        # The target mask ensures that the model won't 'see' future elements of the sequence\n",
    "        return self.decoder(tgt, encoder_output, src_mask, tgt_mask)\n",
    "\n",
    "    # Applying Projection Layer with the Softmax function to the Decoder output\n",
    "    def project(self, x):\n",
    "        return self.projection_layer(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e04bcef7-63ee-4e9d-babe-780f9d0e9ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building & Initializing Transformer\n",
    "\n",
    "# Definin function and its parameter, including model dimension, number of encoder and decoder stacks, heads, etc.\n",
    "def build_transformer(src_vocab_size: int, tgt_vocab_size: int, src_seq_len: int, tgt_seq_len: int, d_model: int = 512, N: int = 6, h: int = 8, dropout: float = 0.1, d_ff: int = 2048) -> Transformer:\n",
    "\n",
    "    # Creating Embedding layers\n",
    "    src_embed = InputEmbeddings(d_model, src_vocab_size) # Source language (Source Vocabulary to 512-dimensional vectors)\n",
    "    tgt_embed = InputEmbeddings(d_model, tgt_vocab_size) # Target language (Target Vocabulary to 512-dimensional vectors)\n",
    "\n",
    "    # Creating Positional Encoding layers\n",
    "    src_pos = PositionalEncoding(d_model, src_seq_len, dropout) # Positional encoding for the source language embeddings\n",
    "    tgt_pos = PositionalEncoding(d_model, tgt_seq_len, dropout) # Positional encoding for the target language embeddings\n",
    "\n",
    "    # Creating EncoderBlocks\n",
    "    encoder_blocks = [] # Initial list of empty EncoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' EncoderBlocks (N = 6)\n",
    "        encoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combine layers into an EncoderBlock\n",
    "        encoder_block = EncoderBlock(encoder_self_attention_block, feed_forward_block, dropout)\n",
    "        encoder_blocks.append(encoder_block) # Appending EncoderBlock to the list of EncoderBlocks\n",
    "\n",
    "    # Creating DecoderBlocks\n",
    "    decoder_blocks = [] # Initial list of empty DecoderBlocks\n",
    "    for _ in range(N): # Iterating 'N' times to create 'N' DecoderBlocks (N = 6)\n",
    "        decoder_self_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Self-Attention\n",
    "        decoder_cross_attention_block = MultiHeadAttentionBlock(d_model, h, dropout) # Cross-Attention\n",
    "        feed_forward_block = FeedForwardBlock(d_model, d_ff, dropout) # FeedForward\n",
    "\n",
    "        # Combining layers into a DecoderBlock\n",
    "        decoder_block = DecoderBlock(decoder_self_attention_block, decoder_cross_attention_block, feed_forward_block, dropout)\n",
    "        decoder_blocks.append(decoder_block) # Appending DecoderBlock to the list of DecoderBlocks\n",
    "\n",
    "    # Creating the Encoder and Decoder by using the EncoderBlocks and DecoderBlocks lists\n",
    "    encoder = Encoder(nn.ModuleList(encoder_blocks))\n",
    "    decoder = Decoder(nn.ModuleList(decoder_blocks))\n",
    "\n",
    "    # Creating projection layer\n",
    "    projection_layer = ProjectionLayer(d_model, tgt_vocab_size) # Map the output of Decoder to the Target Vocabulary Space\n",
    "\n",
    "    # Creating the transformer by combining everything above\n",
    "    transformer = Transformer(encoder, decoder, src_embed, tgt_embed, src_pos, tgt_pos, projection_layer)\n",
    "\n",
    "    # Initialize the parameters\n",
    "    for p in transformer.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "\n",
    "    return transformer # Assembled and initialized Transformer. Ready to be trained and validated!\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "7d81a95f-19c7-488c-810d-8f7dbd0f668c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining Tokenizer\n",
    "def build_tokenizer(config, ds, lang):\n",
    "\n",
    "    # Crating a file path for the tokenizer\n",
    "    tokenizer_path = Path(config['tokenizer_file'].format(lang))\n",
    "\n",
    "    # Checking if Tokenizer already exists\n",
    "    if not Path.exists(tokenizer_path):\n",
    "\n",
    "        # If it doesn't exist, we create a new one\n",
    "        tokenizer = Tokenizer(WordLevel(unk_token = '[UNK]')) # Initializing a new world-level tokenizer\n",
    "        tokenizer.pre_tokenizer = Whitespace() # We will split the text into tokens based on whitespace\n",
    "\n",
    "        # Creating a trainer for the new tokenizer\n",
    "        trainer = WordLevelTrainer(special_tokens = [\"[UNK]\", \"[PAD]\",\n",
    "                                                     \"[SOS]\", \"[EOS]\"], min_frequency = 2) # Defining Word Level strategy and special tokens\n",
    "\n",
    "        # Training new tokenizer on sentences from the dataset and language specified\n",
    "        tokenizer.train_from_iterator(get_all_sentences(ds, lang), trainer = trainer)\n",
    "        tokenizer.save(str(tokenizer_path)) # Saving trained tokenizer to the file path specified at the beginning of the function\n",
    "    else:\n",
    "        tokenizer = Tokenizer.from_file(str(tokenizer_path)) # If the tokenizer already exist, we load it\n",
    "    return tokenizer # Returns the loaded tokenizer or the trained tokenizer\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "cec6b8c2-f57f-466a-b25d-b5b652767e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterating through dataset to extract the original sentence and its translation\n",
    "def get_all_sentences(ds, lang):\n",
    "    for pair in ds:\n",
    "        yield pair['translation'][lang]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "65db2f1f-62c2-4e0b-84c9-c3f5a26ba0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_ds(config):\n",
    "\n",
    "    # Loading the train portion of the Opus-100 dataset.\n",
    "    # The Language pairs will be defined in the 'config' dictionary we will build later\n",
    "    # Load ONLY the train split directly — returns a plain Dataset, NOT a DatasetDict.\n",
    "    ds_raw = load_dataset(\n",
    "        \"parquet\",\n",
    "        data_files=\"test-00000-of-00001.parquet\",\n",
    "        split=\"train\"\n",
    "    )\n",
    "    \n",
    "    # Building or loading tokenizer for both the source and target languages\n",
    "    tokenizer_src = build_tokenizer(config, ds_raw, config['lang_src'])\n",
    "    tokenizer_tgt = build_tokenizer(config, ds_raw, config['lang_tgt'])\n",
    "\n",
    "    # Splitting the dataset for training and validation\n",
    "    train_ds_size = int(0.9 * len(ds_raw)) # 90% for training\n",
    "    val_ds_size = len(ds_raw) - train_ds_size # 10% for validation\n",
    "    train_ds_raw, val_ds_raw = random_split(ds_raw, [train_ds_size, val_ds_size]) # Randomly splitting the dataset\n",
    "\n",
    "    # Processing data with the BilingualDataset class, which we will define below\n",
    "    train_ds = BilingualDataset(train_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "    val_ds = BilingualDataset(val_ds_raw, tokenizer_src, tokenizer_tgt, config['lang_src'], config['lang_tgt'], config['seq_len'])\n",
    "\n",
    "    # Iterating over the entire dataset and printing the maximum length found in the sentences of both the source and target languages\n",
    "    max_len_src = 0\n",
    "    max_len_tgt = 0\n",
    "    for pair in ds_raw:\n",
    "        src_ids = tokenizer_src.encode(pair['translation'][config['lang_src']]).ids\n",
    "        tgt_ids = tokenizer_src.encode(pair['translation'][config['lang_tgt']]).ids\n",
    "        max_len_src = max(max_len_src, len(src_ids))\n",
    "        max_len_tgt = max(max_len_tgt, len(tgt_ids))\n",
    "\n",
    "    print(f'Max length of source sentence: {max_len_src}')\n",
    "    print(f'Max length of target sentence: {max_len_tgt}')\n",
    "\n",
    "    # Creating dataloaders for the training and validadion sets\n",
    "    # Dataloaders are used to iterate over the dataset in batches during training and validation\n",
    "    train_dataloader = DataLoader(train_ds, batch_size = config['batch_size'], shuffle = True) # Batch size will be defined in the config dictionary\n",
    "    val_dataloader = DataLoader(val_ds, batch_size = 1, shuffle = True)\n",
    "\n",
    "    return train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt # Returning the DataLoader objects and tokenizers\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ef6cea0-1fdd-4234-918c-f5a38d4c0b24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "4acb0134-35ab-4601-9ea6-a26040d32c1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def casual_mask(size):\n",
    "        # Creating a square matrix of dimensions 'size x size' filled with ones\n",
    "        mask = torch.triu(torch.ones(1, size, size), diagonal = 1).type(torch.int)\n",
    "        return mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "223f4f73-e81b-4882-8441-31d417649aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BilingualDataset(Dataset):\n",
    "\n",
    "    # This takes in the dataset contaning sentence pairs, the tokenizers for target and source languages, and the strings of source and target languages\n",
    "    # 'seq_len' defines the sequence length for both languages\n",
    "    def __init__(self, ds, tokenizer_src, tokenizer_tgt, src_lang, tgt_lang, seq_len) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.seq_len = seq_len\n",
    "        self.ds = ds\n",
    "        self.tokenizer_src = tokenizer_src\n",
    "        self.tokenizer_tgt = tokenizer_tgt\n",
    "        self.src_lang = src_lang\n",
    "        self.tgt_lang = tgt_lang\n",
    "\n",
    "        # Defining special tokens by using the target language tokenizer\n",
    "        self.sos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[SOS]\")], dtype=torch.int64)\n",
    "        self.eos_token = torch.tensor([tokenizer_tgt.token_to_id(\"[EOS]\")], dtype=torch.int64)\n",
    "        self.pad_token = torch.tensor([tokenizer_tgt.token_to_id(\"[PAD]\")], dtype=torch.int64)\n",
    "\n",
    "\n",
    "    # Total number of instances in the dataset (some pairs are larger than others)\n",
    "    def __len__(self):\n",
    "        return len(self.ds)\n",
    "\n",
    "    # Using the index to retrive source and target texts\n",
    "    def __getitem__(self, index: Any) -> Any:\n",
    "        src_target_pair = self.ds[index]\n",
    "        src_text = src_target_pair['translation'][self.src_lang]\n",
    "        tgt_text = src_target_pair['translation'][self.tgt_lang]\n",
    "\n",
    "        # Tokenizing source and target texts\n",
    "        enc_input_tokens = self.tokenizer_src.encode(src_text).ids\n",
    "        dec_input_tokens = self.tokenizer_tgt.encode(tgt_text).ids\n",
    "\n",
    "        # Computing how many padding tokens need to be added to the tokenized texts\n",
    "        # Source tokens\n",
    "        enc_num_padding_tokens = self.seq_len - len(enc_input_tokens) - 2 # Subtracting the two '[EOS]' and '[SOS]' special tokens\n",
    "        # Target tokens\n",
    "        dec_num_padding_tokens = self.seq_len - len(dec_input_tokens) - 1 # Subtracting the '[SOS]' special token\n",
    "\n",
    "        # If the texts exceed the 'seq_len' allowed, it will raise an error. This means that one of the sentences in the pair is too long to be processed\n",
    "        # given the current sequence length limit (this will be defined in the config dictionary below)\n",
    "        if enc_num_padding_tokens < 0 or dec_num_padding_tokens < 0:\n",
    "            raise ValueError('Sentence is too long')\n",
    "\n",
    "        # Building the encoder input tensor by combining several elements\n",
    "        encoder_input = torch.cat(\n",
    "            [\n",
    "            self.sos_token, # inserting the '[SOS]' token\n",
    "            torch.tensor(enc_input_tokens, dtype = torch.int64), # Inserting the tokenized source text\n",
    "            self.eos_token, # Inserting the '[EOS]' token\n",
    "            torch.tensor([self.pad_token] * enc_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Building the decoder input tensor by combining several elements\n",
    "        decoder_input = torch.cat(\n",
    "            [\n",
    "                self.sos_token, # inserting the '[SOS]' token\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Addind padding tokens\n",
    "            ]\n",
    "\n",
    "        )\n",
    "\n",
    "        # Creating a label tensor, the expected output for training the model\n",
    "        label = torch.cat(\n",
    "            [\n",
    "                torch.tensor(dec_input_tokens, dtype = torch.int64), # Inserting the tokenized target text\n",
    "                self.eos_token, # Inserting the '[EOS]' token\n",
    "                torch.tensor([self.pad_token] * dec_num_padding_tokens, dtype = torch.int64) # Adding padding tokens\n",
    "\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # Ensuring that the length of each tensor above is equal to the defined 'seq_len'\n",
    "        assert encoder_input.size(0) == self.seq_len\n",
    "        assert decoder_input.size(0) == self.seq_len\n",
    "        assert label.size(0) == self.seq_len\n",
    "\n",
    "        return {\n",
    "            'encoder_input': encoder_input,\n",
    "            'decoder_input': decoder_input,\n",
    "            'encoder_mask': (encoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int(),\n",
    "            'decoder_mask': (decoder_input != self.pad_token).unsqueeze(0).unsqueeze(0).int() & casual_mask(decoder_input.size(0)),\n",
    "            'label': label,\n",
    "            'src_text': src_text,\n",
    "            'tgt_text': tgt_text\n",
    "        }\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "b9d21108-484f-4e56-9941-4aba6454cf54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to obtain the most probable next token\n",
    "def greedy_decode(model, source, source_mask, tokenizer_src, tokenizer_tgt, max_len, device):\n",
    "    # Retrieving the indices from the start and end of sequences of the target tokens\n",
    "    sos_idx = tokenizer_tgt.token_to_id('[SOS]')\n",
    "    eos_idx = tokenizer_tgt.token_to_id('[EOS]')\n",
    "\n",
    "    # Computing the output of the encoder for the source sequence\n",
    "    encoder_output = model.encode(source, source_mask)\n",
    "    # Initializing the decoder input with the Start of Sentence token\n",
    "    decoder_input = torch.empty(1,1).fill_(sos_idx).type_as(source).to(device)\n",
    "\n",
    "    # Looping until the 'max_len', maximum length, is reached\n",
    "    while True:\n",
    "        if decoder_input.size(1) == max_len:\n",
    "            break\n",
    "\n",
    "        # Building a mask for the decoder input\n",
    "        decoder_mask = casual_mask(decoder_input.size(1)).type_as(source_mask).to(device)\n",
    "\n",
    "        # Calculating the output of the decoder\n",
    "        out = model.decode(encoder_output, source_mask, decoder_input, decoder_mask)\n",
    "\n",
    "        # Applying the projection layer to get the probabilities for the next token\n",
    "        prob = model.project(out[:, -1])\n",
    "\n",
    "        # Selecting token with the highest probability\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        decoder_input = torch.cat([decoder_input, torch.empty(1,1). type_as(source).fill_(next_word.item()).to(device)], dim=1)\n",
    "\n",
    "        # If the next token is an End of Sentence token, we finish the loop\n",
    "        if next_word == eos_idx:\n",
    "            break\n",
    "\n",
    "    return decoder_input.squeeze(0) # Sequence of tokens generated by the decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "b9dc83fd-eda2-44af-8ed7-58f981335a6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining function to evaluate the model on the validation dataset\n",
    "# num_examples = 2, two examples per run\n",
    "def run_validation(model, validation_ds, tokenizer_src, tokenizer_tgt, max_len, device, print_msg, global_state, writer, num_examples=2):\n",
    "    model.eval() # Setting model to evaluation mode\n",
    "    count = 0 # Initializing counter to keep track of how many examples have been processed\n",
    "\n",
    "    console_width = 80 # Fixed witdh for printed messages\n",
    "\n",
    "    # Creating evaluation loop\n",
    "    with torch.no_grad(): # Ensuring that no gradients are computed during this process\n",
    "        for batch in validation_ds:\n",
    "            count += 1\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "\n",
    "            # Ensuring that the batch_size of the validation set is 1\n",
    "            assert encoder_input.size(0) ==  1, 'Batch size must be 1 for validation.'\n",
    "\n",
    "            # Applying the 'greedy_decode' function to get the model's output for the source text of the input batch\n",
    "            model_out = greedy_decode(model, encoder_input, encoder_mask, tokenizer_src, tokenizer_tgt, max_len, device)\n",
    "\n",
    "            # Retrieving source and target texts from the batch\n",
    "            source_text = batch['src_text'][0]\n",
    "            target_text = batch['tgt_text'][0] # True translation\n",
    "            model_out_text = tokenizer_tgt.decode(model_out.detach().cpu().numpy()) # Decoded, human-readable model output\n",
    "\n",
    "            # Printing results\n",
    "            print_msg('-'*console_width)\n",
    "            print_msg(f'SOURCE: {source_text}')\n",
    "            print_msg(f'TARGET: {target_text}')\n",
    "            print_msg(f'PREDICTED: {model_out_text}')\n",
    "\n",
    "            # After two examples, we break the loop\n",
    "            if count == num_examples:\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "1818b72a-5df0-46d7-99ed-10e5afedba34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We pass as parameters the config dictionary, the length of the vocabylary of the source language and the target language\n",
    "def get_model(config, vocab_src_len, vocab_tgt_len):\n",
    "\n",
    "    # Loading model using the 'build_transformer' function.\n",
    "    # We will use the lengths of the source language and target language vocabularies, the 'seq_len', and the dimensionality of the embeddings\n",
    "    model = build_transformer(vocab_src_len, vocab_tgt_len, config['seq_len'], config['seq_len'], config['d_model'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "fd19d8ba-def3-419e-838f-5d21fd8b1a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define settings for building and training the transformer model\n",
    "def get_config():\n",
    "    return{\n",
    "        'batch_size': 8,\n",
    "        'num_epochs': 2,\n",
    "        'lr': 10**-7,\n",
    "        'seq_len': 250,\n",
    "        'd_model': 512, # Dimensions of the embeddings in the Transformer. 512 like in the \"Attention Is All You Need\" paper.\n",
    "        'lang_src': 'en',\n",
    "        'lang_tgt': 'hi',\n",
    "        'model_folder': 'weights',\n",
    "        'model_basename': 'tmodel_',\n",
    "        'preload': None,\n",
    "        'tokenizer_file': 'tokenizer_{0}.json',\n",
    "        'experiment_name': 'runs/tmodel'\n",
    "    }\n",
    "\n",
    "\n",
    "# Function to construct the path for saving and retrieving model weights\n",
    "def get_weights_file_path(config, epoch: str):\n",
    "    model_folder = config['model_folder'] # Extracting model folder from the config\n",
    "    model_basename = config['model_basename'] # Extracting the base name for model files\n",
    "    model_filename = f\"{model_basename}{epoch}.pt\" # Building filename\n",
    "    return str(Path('.')/ model_folder/ model_filename) # Combining current directory, the model folder, and the model filename\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "5b9ece76-dfb4-4b18-a0ce-3a05f1bbac54",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(config):\n",
    "    # Setting up device to run on GPU to train faster\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    print(f\"Using device {device}\")\n",
    "\n",
    "    # Creating model directory to store weights\n",
    "    Path(config['model_folder']).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Retrieving dataloaders and tokenizers for source and target languages using the 'get_ds' function\n",
    "    train_dataloader, val_dataloader, tokenizer_src, tokenizer_tgt = get_ds(config)\n",
    "\n",
    "    # Initializing model on the GPU using the 'get_model' function\n",
    "    model = get_model(config,tokenizer_src.get_vocab_size(), tokenizer_tgt.get_vocab_size()).to(device)\n",
    "\n",
    "    # Tensorboard\n",
    "    writer = SummaryWriter(config['experiment_name'])\n",
    "\n",
    "    # Setting up the Adam optimizer with the specified learning rate from the '\n",
    "    # config' dictionary plus an epsilon value\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=config['lr'], eps = 1e-9)\n",
    "\n",
    "    # Initializing epoch and global step variables\n",
    "    initial_epoch = 0\n",
    "    global_step = 0\n",
    "\n",
    "    # Checking if there is a pre-trained model to load\n",
    "    # If true, loads it\n",
    "    if config['preload']:\n",
    "        model_filename = get_weights_file_path(config, config['preload'])\n",
    "        print(f'Preloading model {model_filename}')\n",
    "        state = torch.load(model_filename) # Loading model\n",
    "\n",
    "        # Sets epoch to the saved in the state plus one, to resume from where it stopped\n",
    "        initial_epoch = state['epoch'] + 1\n",
    "        # Loading the optimizer state from the saved model\n",
    "        optimizer.load_state_dict(state['optimizer_state_dict'])\n",
    "        # Loading the global step state from the saved model\n",
    "        global_step = state['global_step']\n",
    "\n",
    "    # Initializing CrossEntropyLoss function for training\n",
    "    # We ignore padding tokens when computing loss, as they are not relevant for the learning process\n",
    "    # We also apply label_smoothing to prevent overfitting\n",
    "    loss_fn = nn.CrossEntropyLoss(ignore_index = tokenizer_src.token_to_id('[PAD]'), label_smoothing = 0.1).to(device)\n",
    "\n",
    "    # Initializing training loop\n",
    "\n",
    "    # Iterating over each epoch from the 'initial_epoch' variable up to\n",
    "    # the number of epochs informed in the config\n",
    "    for epoch in range(initial_epoch, config['num_epochs']):\n",
    "\n",
    "        # Initializing an iterator over the training dataloader\n",
    "        # We also use tqdm to display a progress bar\n",
    "        batch_iterator = tqdm(train_dataloader, desc = f'Processing epoch {epoch:02d}')\n",
    "\n",
    "        # For each batch...\n",
    "        for batch in batch_iterator:\n",
    "            model.train() # Train the model\n",
    "\n",
    "            # Loading input data and masks onto the GPU\n",
    "            encoder_input = batch['encoder_input'].to(device)\n",
    "            decoder_input = batch['decoder_input'].to(device)\n",
    "            encoder_mask = batch['encoder_mask'].to(device)\n",
    "            decoder_mask = batch['decoder_mask'].to(device)\n",
    "\n",
    "            # Running tensors through the Transformer\n",
    "            encoder_output = model.encode(encoder_input, encoder_mask)\n",
    "            decoder_output = model.decode(encoder_output, encoder_mask, decoder_input, decoder_mask)\n",
    "            proj_output = model.project(decoder_output)\n",
    "\n",
    "            # Loading the target labels onto the GPU\n",
    "            label = batch['label'].to(device)\n",
    "\n",
    "            # Computing loss between model's output and true labels\n",
    "            loss = loss_fn(proj_output.view(-1, tokenizer_tgt.get_vocab_size()), label.view(-1))\n",
    "\n",
    "            # Updating progress bar\n",
    "            batch_iterator.set_postfix({f\"loss\": f\"{loss.item():6.3f}\"})\n",
    "\n",
    "            writer.add_scalar('train loss', loss.item(), global_step)\n",
    "            writer.flush()\n",
    "\n",
    "            # Performing backpropagation\n",
    "            loss.backward()\n",
    "\n",
    "            # Updating parameters based on the gradients\n",
    "            optimizer.step()\n",
    "\n",
    "            # Clearing the gradients to prepare for the next batch\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            global_step += 1 # Updating global step count\n",
    "\n",
    "        # We run the 'run_validation' function at the end of each epoch\n",
    "        # to evaluate model performance\n",
    "        run_validation(model, val_dataloader, tokenizer_src, tokenizer_tgt, config['seq_len'], device, lambda msg: batch_iterator.write(msg), global_step, writer)\n",
    "\n",
    "        # Saving model\n",
    "        model_filename = get_weights_file_path(config, f'{epoch:02d}')\n",
    "        # Writting current model state to the 'model_filename'\n",
    "        torch.save({\n",
    "            'epoch': epoch, # Current epoch\n",
    "            'model_state_dict': model.state_dict(),# Current model state\n",
    "            'optimizer_state_dict': optimizer.state_dict(), # Current optimizer state\n",
    "            'global_step': global_step # Current global step\n",
    "        }, model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 402,
   "id": "1a80fbc6-f4a4-416f-b89e-8bdc6dc0469d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device cpu\n",
      "Max length of source sentence: 189\n",
      "Max length of target sentence: 207\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 00: 100%|██| 225/225 [44:11<00:00, 11.78s/it, loss=10.334]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Simulation of image to CD burning\n",
      "TARGET: छवि की सीडी बर्निंग का सिमुलेशन\n",
      "PREDICTED: स्नातक क्रिप्टोग्राफी रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न स्नातक क्रिप्टोग्राफी स्नातक क्रिप्टोग्राफी रत्न क्रिप्टोग्राफी रत्न क्रिप्टोग्राफी रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न रत्न अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस स्नातक रत्न रत्न रत्न रत्न स्नातक अफ़सोस अफ़सोस अफ़सोस नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज अफ़सोस अफ़सोस अफ़सोस स्नातक स्नातक स्नातक स्नातक स्नातक अफ़सोस स्नातक रिकार्डिंग स्नातक रिकार्डिंग स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक रिकार्डिंग स्नातक अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस नजरअंदाज नजरअंदाज नजरअंदाज अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज स्नातक यापन दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों स्नातक यापन दिखानेवालों अफ़सोस अफ़सोस अफ़सोस रिकार्डिंग उमराओ यापन नजरअंदाज यापन यापन यापन यापन यापन अफ़सोस अफ़सोस अफ़सोस अफ़सोस रत्न अफ़सोस रत्न अफ़सोस रत्न अफ़सोस रत्न नजरअंदाज रत्न अफ़सोस अफ़सोस अफ़सोस अफ़सोस यापन नजरअंदाज यापन यापन यापन यापन यापन यापन यापन दिखानेवालों यापन दिखानेवालों यापन यापन यापन अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस रत्न रत्न रत्न यापन नजरअंदाज अफ़सोस अफ़सोस अफ़सोस रत्न अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस रत्न रत्न रत्न रत्न रत्न रत्न अफ़सोस रत्न अफ़सोस यापन यापन यापन यापन यापन यापन यापन यापन नजरअंदाज यापन नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस यापन नजरअंदाज यापन यापन अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस अफ़सोस\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: Commit on fast-forward merges\n",
      "TARGET: कमिट पर तेज आगे\n",
      "PREDICTED: स्नातक क्रिप्टोग्राफी स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक क्रिप्टोग्राफी स्नातक क्रिप्टोग्राफी क्रिप्टोग्राफी क्रिप्टोग्राफी रत्न स्नातक रत्न स्नातक क्रिप्टोग्राफी रत्न क्रिप्टोग्राफी रत्न नजरअंदाज रत्न रत्न रत्न दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों रत्न रत्न क्रिप्टोग्राफी चुनांन्चे उमराओ नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक साफ़ स्नातक साफ़ स्नातक नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज नजरअंदाज स्नातक यापन नजरअंदाज यापन नजरअंदाज साफ़ यापन साफ़ यापन यापन यापन नजरअंदाज नजरअंदाज स्नातक साफ़ स्नातक यापन यापन दिखानेवालों स्नातक यापन नजरअंदाज साफ़ यापन साफ़ यापन साफ़ साफ़ साफ़ साफ़ साफ़ यापन यापन यापन यापन यापन यापन चटिया साफ़ यापन नजरअंदाज यापन यापन उमराओ उमराओ उमराओ यापन नजरअंदाज नजरअंदाज उमराओ स्नातक चटिया नजरअंदाज यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन नजरअंदाज यापन नजरअंदाज यापन नजरअंदाज अफ़सोस अफ़सोस अफ़सोस यापन नजरअंदाज रत्न नजरअंदाज रत्न यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन चटिया यापन यापन यापन यापन यापन नजरअंदाज यापन नजरअंदाज यापन यापन -( यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन चटिया चटिया चटिया\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing epoch 01: 100%|██| 225/225 [44:47<00:00, 11.95s/it, loss=10.276]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------\n",
      "SOURCE: we wanted you to learn what it meant to be a human first.\n",
      "TARGET: हम तुम क्या सीखना चाहता था यह एक मानव पहले होने का मतलब.\n",
      "PREDICTED: स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक क्रिप्टोग्राफी स्नातक स्नातक स्नातक स्नातक स्नातक रत्न स्नातक रत्न स्नातक रत्न रत्न रत्न रत्न स्नातक पतंगों पतंगों पतंगों पतंगों पतंगों पतंगों स्नातक स्नातक स्नातक प्रकाश स्नातक प्रकाश स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक उमराओ स्नातक यापन यापन स्नातक यापन उमराओ स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक उमराओ उमराओ स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक यापन यापन पतंगों स्नातक स्नातक स्नातक स्नातक स्नातक यापन स्नातक स्नातक स्नातक स्नातक यापन यापन यापन यापन स्नातक स्नातक रत्न स्नातक यापन यापन यापन रत्न स्नातक यापन रत्न यापन यापन यापन रत्न स्नातक रत्न स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक रत्न रत्न रत्न स्नातक स्नातक स्नातक स्नातक स्नातक यापन यापन यापन प्रकाश स्नातक यापन स्नातक स्नातक स्नातक स्नातक यापन प्रकाश स्नातक प्रकाश स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक यापन यापन यापन यापन यापन रत्न स्नातक रत्न स्नातक रत्न\n",
      "--------------------------------------------------------------------------------\n",
      "SOURCE: We should go get in a batting cage.\n",
      "TARGET: हम बल्लेबाजी के पिंजरे में मिल जाना चाहिए.\n",
      "PREDICTED: स्नातक स्नातक क्रिप्टोग्राफी स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक क्रिप्टोग्राफी स्नातक क्रिप्टोग्राफी क्रिप्टोग्राफी यापन यापन यापन यापन ठहराव ठहराव ठहराव यापन यापन यापन यापन दिखानेवालों दिखानेवालों दिखानेवालों दिखानेवालों यापन यापन दिखानेवालों यापन दिखानेवालों यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन दिखानेवालों उमराओ उमराओ उमराओ रत्न स्नातक डिजायनर स्नातक यापन स्नातक यापन नजरअंदाज यापन दिखानेवालों स्नातक यापन यापन स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक यापन स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक स्नातक यापन यापन यापन उमराओ उमराओ उमराओ यापन प्रकाश यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन यापन\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    warnings.filterwarnings('ignore') # Filtering warnings\n",
    "    config = get_config() # Retrieving config settings\n",
    "    train_model(config) # Training model with the config arguments"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
